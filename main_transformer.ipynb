{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_transformer.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Ko5H3RRTgcnDvc6R3aNtz9MJW2V4ACUR","authorship_tag":"ABX9TyNSyCE0McDAD7gXs3ISyKXS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D4GWINL5AxHO","executionInfo":{"status":"ok","timestamp":1656252993308,"user_tz":300,"elapsed":138,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}},"outputId":"990819cd-8937-4e2b-fee3-5f73fc77b4b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/dev/aiac/TRBAIAC-TRAFFIC\n"]}],"source":["%cd '/content/drive/MyDrive/dev/aiac/TRBAIAC-TRAFFIC'"]},{"cell_type":"code","source":["# Import Packages\n","import torch, os\n","import torch.nn as nn\n","import torch.utils.data as utils\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import time\n","from torch.autograd import Variable\n","import math\n","import copy"],"metadata":{"id":"3CSKp-wGA4jZ","executionInfo":{"status":"ok","timestamp":1656253837955,"user_tz":300,"elapsed":177,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Download training, testing and shapefile data\n","\n","!gdown 'https://drive.google.com/uc?id=1E_qqe7kfvfApM4hCOBMPoXhyEPyrUJkN'\n","!gdown 'https://drive.google.com/uc?id=1j-3-lHegY--FDHKZvz86HBtilV2dsP-i'\n","!gdown 'https://drive.google.com/uc?id=1AhAb3yEVxcHSaz0ADlyyYwNH2Gqmr-um'\n","!unzip 'train.zip' -d '.'\n","!unzip 'test.zip' -d '.'\n","!unzip 'Geoshapefile.zip' -d '.'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CCZ1ASv9a-ap","executionInfo":{"status":"ok","timestamp":1656254346411,"user_tz":300,"elapsed":3364,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}},"outputId":"f3a218e4-feb5-4ab6-d5ac-f7251c3e84f3"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1j-3-lHegY--FDHKZvz86HBtilV2dsP-i\n","To: /content/drive/MyDrive/dev/aiac/TRBAIAC-TRAFFIC/test.zip\n","100% 459k/459k [00:00<00:00, 73.0MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1AhAb3yEVxcHSaz0ADlyyYwNH2Gqmr-um\n","To: /content/drive/MyDrive/dev/aiac/TRBAIAC-TRAFFIC/Geoshapefile.zip\n","100% 53.4k/53.4k [00:00<00:00, 61.0MB/s]\n","Archive:  test.zip\n","  inflating: ./test_data/tps_5.pkl   \n","  inflating: ./test_data/tps_2.pkl   \n","  inflating: ./test_data/tps_6.pkl   \n","  inflating: ./test_data/tps_4.pkl   \n","  inflating: ./test_data/tps_15.pkl  \n","  inflating: ./test_data/tps_7.pkl   \n","  inflating: ./test_data/tps_1.pkl   \n","  inflating: ./test_data/tps_12.pkl  \n","  inflating: ./test_data/tps_8.pkl   \n","  inflating: ./test_data/tps_3.pkl   \n","  inflating: ./test_data/tps_9.pkl   \n","  inflating: ./test_data/tps_13.pkl  \n","  inflating: ./test_data/tps_11.pkl  \n","  inflating: ./test_data/tps_10.pkl  \n","  inflating: ./test_data/tps_14.pkl  \n","Archive:  Geoshapefile.zip\n","  inflating: ./Geoshapefile/hackathon.cpg  \n","  inflating: ./Geoshapefile/hackathon.shx  \n","  inflating: ./Geoshapefile/hackathon.prj  \n","  inflating: ./Geoshapefile/hackathon.dbf  \n","  inflating: ./Geoshapefile/hackathon.shp  \n"]}]},{"cell_type":"code","source":["# Data Preparation\n","def reshape_data(rawdata):\n","    reshaped_tps_df = pd.DataFrame()\n","    reshaped_tps_df['TIME'] = rawdata.time.unique()\n","    for seg in rawdata.segmentID.unique():\n","        column = rawdata[rawdata['segmentID'] == seg][['time','TrafficIndex_GP']].drop_duplicates(subset=['time'])\n","        column.columns = ['TIME', str(seg)]\n","        reshaped_tps_df = reshaped_tps_df.join(column.set_index('TIME'), on='TIME')\n","\n","    return reshaped_tps_df\n","\n","def load_data(filepath, start_time, end_time, freq):\n","    rawdata = pd.read_pickle(filepath)\n","    matrix = reshape_data(rawdata)\n","    matrix['TIME'] = pd.to_datetime(matrix['TIME']).dt.strftime('%Y-%m-%d %H:%M:%S')\n","\n","    dt_idx = pd.date_range(start=start_time, end=end_time, freq=freq)\n","\n","    output = pd.DataFrame(dt_idx)\n","    output.columns = ['TIME']\n","    output['TIME'] = pd.to_datetime(output['TIME']).dt.strftime('%Y-%m-%d %H:%M:%S')\n","    output = output.set_index('TIME').join(matrix.set_index('TIME'))\n","\n","    return output\n","\n","def prepare_dataloader(matrix, n_col, seq_len=36, pred_len=12, BATCH_SIZE=32, device='cpu'):\n","    seg = matrix.columns.values\n","    time = matrix.index.values\n","    n_seg = len(seg)\n","    n_time = len(time)\n","    \n","    speedMatrix = matrix.to_numpy()\n","    \n","    data_set = []\n","    label_set = []\n","\n","    for i in range(n_time - seq_len - pred_len):\n","        data = speedMatrix[i : i + seq_len]\n","        \n","        label_data = speedMatrix[i + seq_len: i + seq_len + pred_len, :n_col]\n","        \n","        if np.isnan(np.sum(data[:n_col])).any() | np.isnan(np.sum(label_data)):\n","            pass\n","        else:\n","\n","            data_set.append(data)\n","            label_set.append(label_data)\n","            \n","    data = np.array(data_set)\n","    label = np.array(label_set)\n","\n","    train_ind = int(len(data)* 0.8)\n","    valid_ind = int(len(data) * 0.9)\n","    test_ind = int(len(data) * 1.0)\n","\n","    X_train = data[: train_ind]\n","    X_valid = data[train_ind : valid_ind]\n","    X_test = data[valid_ind : test_ind]\n","    Y_train = label[: train_ind]\n","    Y_valid = label[train_ind : valid_ind]\n","    Y_test = label[valid_ind : test_ind]\n","\n","    X_train = torch.FloatTensor(X_train).to(device)\n","    X_valid = torch.FloatTensor(X_valid).to(device)\n","    X_test = torch.FloatTensor(X_test).to(device)\n","    Y_train = torch.FloatTensor(Y_train).to(device)\n","    Y_valid = torch.FloatTensor(Y_valid).to(device)\n","    Y_test = torch.FloatTensor(Y_test).to(device)\n","\n","    train_dataset = utils.TensorDataset(X_train, Y_train)\n","    valid_dataset = utils.TensorDataset(X_valid, Y_valid)\n","    test_dataset = utils.TensorDataset(X_test, Y_test)\n","\n","    train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n","    valid_dataloader = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n","    test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False, drop_last = False)\n","\n","    return train_dataloader, valid_dataloader, test_dataloader"],"metadata":{"id":"GzU3aKNRA4mQ","executionInfo":{"status":"ok","timestamp":1656247584174,"user_tz":300,"elapsed":516,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["'''\n","Revised by https://github.com/gordicaleksa/pytorch-original-transformer/blob/main/models/definitions/transformer_model.py\n","'''\n","# Encoder Architecture\n","class Encoder(nn.Module):\n","\n","    def __init__(self, encoder_layer, number_of_layers):\n","        super().__init__()\n","        assert isinstance(encoder_layer, EncoderLayer), f'Expected EncoderLayer got {type(encoder_layer)}.'\n","        self.encoder_layers = get_clones(encoder_layer, number_of_layers)\n","        self.norm = nn.LayerNorm(encoder_layer.model_dimension)\n","\n","    def forward(self, src_embeddings_batch, src_mask):\n","        src_representations_batch = src_embeddings_batch\n","        for encoder_layer in self.encoder_layers:\n","            src_representations_batch = encoder_layer(src_representations_batch, src_mask)\n","        return self.norm(src_representations_batch)\n","\n","class EncoderLayer(nn.Module):\n","\n","    def __init__(self, model_dimension, dropout_probability, multi_headed_attention, pointwise_net):\n","        super().__init__()\n","        num_of_sublayers_encoder = 2\n","        self.sublayers = get_clones(SublayerLogic(model_dimension, dropout_probability), num_of_sublayers_encoder)\n","\n","        self.multi_headed_attention = multi_headed_attention\n","        self.pointwise_net = pointwise_net\n","\n","        self.model_dimension = model_dimension\n","\n","    def forward(self, src_representations_batch, src_mask):\n","        encoder_self_attention = lambda srb: self.multi_headed_attention(query=srb, key=srb, value=srb, mask=src_mask)\n","        src_representations_batch = self.sublayers[0](src_representations_batch, encoder_self_attention)\n","        src_representations_batch = self.sublayers[1](src_representations_batch, self.pointwise_net)\n","\n","        return src_representations_batch\n","\n","# Decoder Architecture\n","class Decoder(nn.Module):\n","\n","    def __init__(self, decoder_layer, number_of_layers):\n","        super().__init__()\n","        assert isinstance(decoder_layer, DecoderLayer), f'Expected DecoderLayer got {type(decoder_layer)}.'\n","\n","        self.decoder_layers = get_clones(decoder_layer, number_of_layers)\n","        self.norm = nn.LayerNorm(decoder_layer.model_dimension)\n","\n","    def forward(self, trg_embeddings_batch, src_representations_batch, trg_mask, src_mask):\n","        trg_representations_batch = trg_embeddings_batch\n","        for decoder_layer in self.decoder_layers:\n","            trg_representations_batch = decoder_layer(trg_representations_batch, src_representations_batch, trg_mask, src_mask)\n","        return self.norm(trg_representations_batch)\n","\n","class DecoderLayer(nn.Module):\n","\n","    def __init__(self, model_dimension, dropout_probability, multi_headed_attention, pointwise_net):\n","        super().__init__()\n","        num_of_sublayers_decoder = 3\n","        self.sublayers = get_clones(SublayerLogic(model_dimension, dropout_probability), num_of_sublayers_decoder)\n","\n","        self.trg_multi_headed_attention = copy.deepcopy(multi_headed_attention)\n","        self.src_multi_headed_attention = copy.deepcopy(multi_headed_attention)\n","        self.pointwise_net = pointwise_net\n","\n","        self.model_dimension = model_dimension\n","\n","    def forward(self, trg_representations_batch, src_representations_batch, trg_mask, src_mask):\n","        srb = src_representations_batch\n","        decoder_trg_self_attention = lambda trb: self.trg_multi_headed_attention(query=trb, key=trb, value=trb, mask=trg_mask)\n","        decoder_src_attention = lambda trb: self.src_multi_headed_attention(query=trb, key=srb, value=srb, mask=src_mask)\n","\n","        trg_representations_batch = self.sublayers[0](trg_representations_batch, decoder_trg_self_attention)\n","        trg_representations_batch = self.sublayers[1](trg_representations_batch, decoder_src_attention)\n","        trg_representations_batch = self.sublayers[2](trg_representations_batch, self.pointwise_net)\n","\n","        return trg_representations_batch"],"metadata":{"id":"UcbSF8WsBJGh","executionInfo":{"status":"ok","timestamp":1656247588249,"user_tz":300,"elapsed":343,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Helper Modules\n","class SublayerLogic(nn.Module):\n","\n","    def __init__(self, model_dimension, dropout_probability):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(model_dimension)\n","        self.dropout = nn.Dropout(p=dropout_probability)\n","\n","    def forward(self, representations_batch, sublayer_module):\n","        return representations_batch + self.dropout(sublayer_module(self.norm(representations_batch)))\n","\n","class DecoderGenerator(nn.Module):\n","\n","    def __init__(self, model_dimension, vocab_size):\n","        super().__init__()\n","        self.linear = nn.Linear(model_dimension, vocab_size)\n","\n","    def forward(self, trg_representations_batch):\n","        return torch.sigmoid(self.linear(trg_representations_batch))\n","\n","class PositionwiseFeedForwardNet(nn.Module):\n","\n","    def __init__(self, model_dimension, dropout_probability, width_mult=4):\n","        super().__init__()\n","        self.linear1 = nn.Linear(model_dimension, width_mult * model_dimension)\n","        self.linear2 = nn.Linear(width_mult * model_dimension, model_dimension)\n","\n","        self.dropout = nn.Dropout(p=dropout_probability)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, representations_batch):\n","        return self.linear2(self.dropout(self.relu(self.linear1(representations_batch))))"],"metadata":{"id":"RjKfG2jzBJIz","executionInfo":{"status":"ok","timestamp":1656247593192,"user_tz":300,"elapsed":160,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Multihead Attention\n","class MultiHeadedAttention(nn.Module):\n","\n","    def __init__(self, model_dimension, number_of_heads, dropout_probability, log_attention_weights):\n","        super().__init__()\n","        assert model_dimension % number_of_heads == 0, f'Model dimension must be divisible by the number of heads.'\n","\n","        self.head_dimension = int(model_dimension / number_of_heads)\n","        self.number_of_heads = number_of_heads\n","\n","        self.qkv_nets = get_clones(nn.Linear(model_dimension, model_dimension), 3)\n","        self.out_projection_net = nn.Linear(model_dimension, model_dimension)\n","\n","        self.attention_dropout = nn.Dropout(p=dropout_probability)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","        self.log_attention_weights = log_attention_weights\n","        self.attention_weights = None\n","\n","    def attention(self, query, key, value, mask):\n","        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dimension)\n","        if mask is not None:\n","            scores.masked_fill_(mask == torch.tensor(False), float(\"-inf\"))\n","\n","        attention_weights = self.softmax(scores)\n","        attention_weights = self.attention_dropout(attention_weights)\n","\n","        intermediate_token_representations = torch.matmul(attention_weights, value)\n","\n","        return intermediate_token_representations, attention_weights\n","\n","    def forward(self, query, key, value, mask):\n","\n","        batch_size = query.shape[0]\n","        query, key, value = [net(x).view(batch_size, -1, self.number_of_heads, self.head_dimension).transpose(1, 2)\n","                             for net, x in zip(self.qkv_nets, (query, key, value))]\n","\n","        intermediate_token_representations, attention_weights = self.attention(query, key, value, mask)\n","\n","        if self.log_attention_weights:\n","            self.attention_weights = attention_weights\n","\n","        reshaped = intermediate_token_representations.transpose(1, 2).reshape(batch_size, -1, self.number_of_heads * self.head_dimension)\n","\n","        token_representations = self.out_projection_net(reshaped)\n","\n","        return token_representations"],"metadata":{"id":"duYVEDvZBJL9","executionInfo":{"status":"ok","timestamp":1656247596769,"user_tz":300,"elapsed":157,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Input Modules\n","class Embedding(nn.Module):\n","\n","    def __init__(self, vocab_size, model_dimension):\n","        super().__init__()\n","        self.embeddings_table = nn.Linear(vocab_size, model_dimension)\n","        self.model_dimension = model_dimension\n","\n","    def forward(self, token_ids_batch):\n","        assert token_ids_batch.ndim == 3, f'Expected: (batch size, max token sequence length), got {token_ids_batch.shape}'\n","        embeddings = self.embeddings_table(token_ids_batch)\n","        return embeddings * math.sqrt(self.model_dimension)\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, model_dimension, dropout_probability, expected_max_sequence_length):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout_probability)\n","\n","        position_id = torch.arange(0, expected_max_sequence_length).unsqueeze(1)\n","        frequencies = torch.pow(10000., -torch.arange(0, model_dimension, 2, dtype=torch.float) / model_dimension)\n","\n","        positional_encodings_table = torch.zeros(expected_max_sequence_length, model_dimension)\n","        positional_encodings_table[:, 0::2] = torch.sin(position_id * frequencies)\n","        positional_encodings_table[:, 1::2] = torch.cos(position_id * frequencies)\n","\n","        self.register_buffer('positional_encodings_table', positional_encodings_table)\n","        self.w2 = nn.Linear(model_dimension * 2, model_dimension)\n","\n","    def forward(self, embeddings_batch):\n","        assert embeddings_batch.ndim == 3 and embeddings_batch.shape[-1] == self.positional_encodings_table.shape[1], \\\n","            f'Expected (batch size, max token sequence length, model dimension) got {embeddings_batch.shape}'\n","\n","        positional_encodings = self.positional_encodings_table[:embeddings_batch.shape[1]]\n","        size_embeddings_batch = embeddings_batch.shape[0]\n","        positional_encodings = positional_encodings.unsqueeze(0).expand(size_embeddings_batch, -1 , -1)\n","        output = self.w2(torch.cat((embeddings_batch, positional_encodings), dim=2))\n","        return self.dropout(output)\n","\n","def get_clones(module, num_of_deep_copies):\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_of_deep_copies)])"],"metadata":{"id":"LLcDUmLgBQnP","executionInfo":{"status":"ok","timestamp":1656247599848,"user_tz":300,"elapsed":149,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Transformer\n","class Transformer(nn.Module):\n","\n","    def __init__(self, model_dimension, input_length, src_vocab_size, trg_vocab_size, number_of_heads, number_of_layers, dropout_probability, log_attention_weights=False):\n","        super().__init__()\n","        self.src_embedding = Embedding(src_vocab_size, model_dimension)\n","        self.trg_embedding = Embedding(trg_vocab_size, model_dimension)\n","        \n","        self.src_pos_embedding = PositionalEncoding(model_dimension, dropout_probability, input_length)\n","        self.trg_pos_embedding = PositionalEncoding(model_dimension, dropout_probability, input_length)\n","\n","        mha = MultiHeadedAttention(model_dimension, number_of_heads, dropout_probability, log_attention_weights)\n","        pwn = PositionwiseFeedForwardNet(model_dimension, dropout_probability)\n","        encoder_layer = EncoderLayer(model_dimension, dropout_probability, mha, pwn)\n","        decoder_layer = DecoderLayer(model_dimension, dropout_probability, mha, pwn)\n","\n","        self.encoder = Encoder(encoder_layer, number_of_layers)\n","        self.decoder = Decoder(decoder_layer, number_of_layers)\n","\n","        self.decoder_generator = DecoderGenerator(model_dimension, trg_vocab_size)\n","        self.init_params()\n","\n","    def init_params(self, default_initialization=False):\n","        if not default_initialization:\n","            for name, p in self.named_parameters():\n","                if p.dim() > 1:\n","                    nn.init.xavier_uniform_(p)\n","\n","    def forward(self, src_token_ids_batch, trg_token_ids_batch, src_mask, trg_mask, num_pred):\n","        batch_size = src_token_ids_batch.shape[0]\n","        num_ahead = num_pred\n","        src_representations_batch = self.encode(src_token_ids_batch, src_mask)\n","        trg_log_probs = self.greedy_decoding(src_representations_batch, trg_token_ids_batch, src_mask, batch_size, num_ahead)\n","        return trg_log_probs\n","\n","    def encode(self, src_token_ids_batch, src_mask):       \n","        src_embeddings_batch = self.src_embedding(src_token_ids_batch)\n","        src_embeddings_batch = self.src_pos_embedding(src_embeddings_batch)\n","        src_representations_batch = self.encoder(src_embeddings_batch, src_mask)\n","\n","        return src_representations_batch\n","\n","    def decode(self, trg_token_ids_batch, src_representations_batch, trg_mask, src_mask):\n","        trg_embeddings_batch = self.trg_embedding(trg_token_ids_batch)\n","        trg_embeddings_batch = self.trg_pos_embedding(trg_embeddings_batch)\n","        trg_representations_batch = self.decoder(trg_embeddings_batch, src_representations_batch, trg_mask, src_mask)\n","        trg_log_probs = self.decoder_generator(trg_representations_batch)\n","        return trg_log_probs\n","    \n","    def greedy_decoding(self, src_representations_batch, inputs_decode, src_mask, batch_size, num_ahead):\n","        \n","        trg_token_ids_batch = inputs_decode[:, -1, :].unsqueeze(1)\n","        for i in range(num_ahead):\n","            trg_mask = get_trg_mask(batch_size, i+1).cuda()\n","            predicted_log_distributions = self.decode(trg_token_ids_batch, src_representations_batch, trg_mask, src_mask)[:,-1:,]\n","            trg_token_ids_batch = torch.cat((trg_token_ids_batch, predicted_log_distributions), 1)\n","\n","        output = trg_token_ids_batch[:,1:, :]\n","        return output\n","\n","def get_src_mask(batch_size, step_size):\n","    x = torch.tensor([True])\n","    src_mask = torch.repeat_interleave(x, step_size*batch_size, dim=0).view(batch_size, 1, 1, -1)\n","    return src_mask\n","\n","def get_trg_mask(batch_size, step_size):\n","    trg_no_look_forward_mask = torch.triu(torch.ones((1, 1, step_size, step_size)) == 1).transpose(2, 3)\n","    x = torch.tensor([True])\n","    trg_padding_mask = torch.repeat_interleave(x, step_size*batch_size, dim=0).view(batch_size, 1, 1, -1)\n","    trg_mask = trg_padding_mask & trg_no_look_forward_mask\n","    return trg_mask"],"metadata":{"id":"ugMm7CMUBVJV","executionInfo":{"status":"ok","timestamp":1656247602378,"user_tz":300,"elapsed":155,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Train Process\n","def TrainModel_Transformer(model, src_mask, trg_mask, train_dataloader, valid_dataloader, num_ahead, num_epochs, learning_rate, patience, min_delta):\n","    \n","    loss_MSE = torch.nn.MSELoss()\n","    optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n","\n","    use_gpu = torch.cuda.is_available()\n","    \n","    cur_time = time.time()\n","    pre_time = time.time()\n","\n","    # Variables for Early Stopping\n","    is_best_model = 0\n","    patient_epoch = 0\n","       \n","    for epoch in range(num_epochs):\n","\n","        total_train_loss = 0\n","        total_valid_loss = 0\n","\n","        for data in train_dataloader:\n","\n","            inputs, labels = data\n","            batch_size = inputs.shape[0]\n","            \n","            if inputs.shape[0] != batch_size:\n","                continue\n","                \n","            if use_gpu:\n","                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n","            else:\n","                inputs, labels = Variable(inputs), Variable(labels)\n","            \n","            outputs = model(inputs, inputs, src_mask, trg_mask, num_ahead)\n","            loss_train = loss_MSE(outputs, torch.squeeze(labels))\n","            total_train_loss += loss_train.data\n","\n","            optimizer.zero_grad()\n","            loss_train.backward()\n","            optimizer.step()\n","        \n","        model.eval()\n","\n","        for data in valid_dataloader:\n","\n","            inputs, labels = data\n","            batch_size = inputs.shape[0]\n","            \n","            if inputs.shape[0] != batch_size:\n","                continue\n","                \n","            if use_gpu:\n","                inputs_val, labels_val = Variable(inputs.cuda()), Variable(labels.cuda())\n","            else:\n","                inputs_val, labels_val = Variable(inputs), Variable(labels)\n","\n","            outputs_val = model(inputs_val, inputs_val, src_mask, trg_mask, num_ahead)\n","            loss_valid = loss_MSE(outputs_val, torch.squeeze(labels_val))\n","            total_valid_loss += loss_valid.data\n","\n","        avg_losses_epoch_train = total_train_loss / float(len(train_dataloader))\n","        avg_losses_epoch_valid = total_valid_loss / float(len(valid_dataloader))\n","\n","        # Early Stopping\n","        if epoch == 0:\n","            is_best_model = 1\n","            best_model = model\n","            min_loss_epoch_valid = 10000.0\n","            if avg_losses_epoch_valid < min_loss_epoch_valid:\n","                min_loss_epoch_valid = avg_losses_epoch_valid\n","        else:\n","            if min_loss_epoch_valid - avg_losses_epoch_valid > min_delta:\n","                is_best_model = 1\n","                best_model = model\n","                min_loss_epoch_valid = avg_losses_epoch_valid\n","                patient_epoch = 0\n","            else:\n","                is_best_model = 0\n","                patient_epoch += 1\n","                if patient_epoch >= patience:\n","                    print('Early Stopped at Epoch:', epoch)\n","                    break\n","\n","\n","        cur_time = time.time()\n","\n","        print('Epoch: {}, train_loss: {}, valid_loss: {}, time: {}, best model: {}'.format( \\\n","                    epoch, \\\n","                    np.around(avg_losses_epoch_train.cpu(), decimals=8),\\\n","                    np.around(avg_losses_epoch_valid.cpu(), decimals=8),\\\n","                    np.around([cur_time - pre_time] , decimals=2),\\\n","                    is_best_model))\n","        pre_time = cur_time\n","\n","    return best_model"],"metadata":{"id":"Uq2MV6HOBYt-","executionInfo":{"status":"ok","timestamp":1656247606715,"user_tz":300,"elapsed":173,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["NUM_COL = 87\n","BATCH_SIZE = 40\n","INPUT_LEN = 36\n","PRED_LEN = 12\n","LEARNING_RATE = 1e-5\n","NUM_EPOCHS = 100\n","MIN_DELTA = 5e-4\n","PATIENCE = 10\n","\n","BASELINE_MODEL_DIMENSION = 512\n","BASELINE_MODEL_NUMBER_OF_HEADS = 8\n","BASELINE_MODEL_NUMBER_OF_LAYERS = 6\n","BASELINE_MODEL_DROPOUT_PROB = 0.3"],"metadata":{"id":"XWRzZ6m4Bcd0","executionInfo":{"status":"ok","timestamp":1656247611326,"user_tz":300,"elapsed":167,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","use_gpu = torch.cuda.is_available()\n","print (use_gpu)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fA6ElK58BnbZ","executionInfo":{"status":"ok","timestamp":1656247620842,"user_tz":300,"elapsed":145,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}},"outputId":"3d412673-8510-458a-ec25-f666e0259f61"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10YJBEh5DYZ_","executionInfo":{"status":"ok","timestamp":1656247653166,"user_tz":300,"elapsed":425,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}},"outputId":"81730e1c-abeb-468c-adbf-0301527c2bff"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n"]}]},{"cell_type":"code","source":["data_matrix = load_data(\"./tps_df.pkl\", \"2020-01-01 00:00:00.000\", \"2020-05-31 23:45:00.000\", freq=\"15min\")\n","input_dim = data_matrix.shape[-1]\n","train_dataloader, valid_dataloader, _ = prepare_dataloader(data_matrix, input_dim, BATCH_SIZE=BATCH_SIZE, seq_len=INPUT_LEN, pred_len=PRED_LEN, device=device)\n","\n","src_mask = get_src_mask(BATCH_SIZE, INPUT_LEN).to(device)\n","trg_mask = get_trg_mask(BATCH_SIZE, PRED_LEN).to(device)\n","\n","transformer = Transformer(\n","    model_dimension=BASELINE_MODEL_DIMENSION, \n","    input_length=INPUT_LEN, \n","    src_vocab_size=NUM_COL, \n","    trg_vocab_size=NUM_COL, \n","    number_of_heads=BASELINE_MODEL_NUMBER_OF_HEADS, \n","    number_of_layers=BASELINE_MODEL_NUMBER_OF_LAYERS, \n","    dropout_probability=BASELINE_MODEL_DROPOUT_PROB\n",").to(device)\n","\n","transformer_best_model = TrainModel_Transformer(transformer, src_mask, trg_mask, train_dataloader, valid_dataloader, num_ahead=PRED_LEN, learning_rate=LEARNING_RATE, min_delta=MIN_DELTA, num_epochs=NUM_EPOCHS, patience=PATIENCE)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tzTTagEaBvU2","outputId":"70269493-c06d-4a6c-f01f-ec7672566e94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, train_loss: 0.019574839621782303, valid_loss: 0.002029919996857643, time: [112.93], best model: 1\n","Epoch: 1, train_loss: 0.009758319705724716, valid_loss: 0.0011948499595746398, time: [100.51], best model: 1\n","Epoch: 2, train_loss: 0.005870840046554804, valid_loss: 0.0009037100244313478, time: [97.74], best model: 0\n","Epoch: 3, train_loss: 0.005188569892197847, valid_loss: 0.0010080799693241715, time: [100.21], best model: 0\n","Epoch: 4, train_loss: 0.004755950067192316, valid_loss: 0.0008686999790370464, time: [99.13], best model: 0\n","Epoch: 5, train_loss: 0.004484639968723059, valid_loss: 0.0007540400256402791, time: [100.55], best model: 0\n","Epoch: 6, train_loss: 0.004312850069254637, valid_loss: 0.000651800015475601, time: [98.67], best model: 1\n","Epoch: 7, train_loss: 0.0039853500202298164, valid_loss: 0.0014126800233498216, time: [99.67], best model: 0\n","Epoch: 8, train_loss: 0.0038302799221128225, valid_loss: 0.0006085400236770511, time: [97.77], best model: 0\n","Epoch: 9, train_loss: 0.0036163399927318096, valid_loss: 0.0006125000072643161, time: [98.78], best model: 0\n","Epoch: 10, train_loss: 0.0034910200629383326, valid_loss: 0.0006825100281275809, time: [100.14], best model: 0\n","Epoch: 11, train_loss: 0.0032795199658721685, valid_loss: 0.0007669000187888741, time: [98.97], best model: 0\n","Epoch: 12, train_loss: 0.0031760300043970346, valid_loss: 0.0007357799913734198, time: [95.71], best model: 0\n","Epoch: 13, train_loss: 0.00299295992590487, valid_loss: 0.0005789599963463843, time: [97.1], best model: 0\n"]}]},{"cell_type":"code","source":["import os "],"metadata":{"id":"UpTnsnilXzTE","executionInfo":{"status":"ok","timestamp":1656253007275,"user_tz":300,"elapsed":3,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["'''\n","Step1: read and reshape data\n","Step2: feed into trained model\n","Step3: convert output to JSON file\n","Here we take one testing data as an example to convert it to JSON file.\n","In this challenge, you should convert all 15 predicting results to one JSON file.\n","Please check https://colab.research.google.com/drive/1Hkt3kQuh7WzwUTnLgKCcvfPAV1CUK8lF?usp=sharing for more information of the expected result.\n","'''\n","\n","test_files = os.listdir('test_data')\n","out_cols = ['11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n","       '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n","       '35', '36', '37', '38', '39', '40', '42', '43', '44', '45', '46', '47',\n","       '49', '50', '51', '52', '53', '54', '55', '58', '59', '60', '61', '62',\n","       '63', '64', '65', '66', '67', '68', '69', '70', '73', '74', '75', '76',\n","       '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n","       '89', '90', '91', '92', '93', '94', '95', '96', '97', '99', '100',\n","       '102', '103', '104', '106']\n","\n","horizon = {'tps_1.pkl':['2020-06-02 06:15:00', '2020-06-02 09:00:00'],\n","           'tps_2.pkl':['2020-06-03 07:15:00', '2020-06-03 10:00:00'],\n","           'tps_3.pkl':['2020-06-04 08:15:00', '2020-06-04 11:00:00'],\n","           'tps_4.pkl':['2020-06-05 09:15:00', '2020-06-05 12:00:00'],\n","           'tps_5.pkl':['2020-06-06 10:15:00', '2020-06-06 13:00:00'],\n","           'tps_6.pkl':['2020-06-07 11:15:00', '2020-06-07 14:00:00'],\n","           'tps_7.pkl':['2020-06-08 12:15:00', '2020-06-08 15:00:00'],\n","           'tps_8.pkl':['2020-06-09 13:15:00', '2020-06-09 16:00:00'],\n","           'tps_9.pkl':['2020-06-10 14:15:00', '2020-06-10 17:00:00'],\n","           'tps_10.pkl':['2020-06-11 15:15:00', '2020-06-11 18:00:00'],\n","           'tps_11.pkl':['2020-06-12 16:15:00', '2020-06-12 19:00:00'],\n","           'tps_12.pkl':['2020-06-13 17:15:00', '2020-06-13 20:00:00'],\n","           'tps_13.pkl':['2020-06-14 18:15:00', '2020-06-14 21:00:00'],\n","           'tps_14.pkl':['2020-06-15 19:15:00', '2020-06-15 22:00:00'],\n","           'tps_15.pkl':['2020-06-16 20:15:00', '2020-06-16 23:00:00'],\n","           }\n","out_df = pd.DataFrame(columns = out_cols)\n","\n","\n","BATCH_SIZE_TEST = 1\n","for test_file in test_files:\n","  print (os.path.join('test_data',test_file))\n","  tps_1_raw = pd.read_pickle(os.path.join('test_data',test_file))\n","\n","  # tps_1_raw = pd.read_pickle('./tps_1.pkl')\n","  reshaped_tps_df = reshape_data(tps_1_raw)\n","  reshaped_tps_df = reshaped_tps_df.set_index('TIME')\n","  reshaped_tps_value = reshaped_tps_df.values\n","  reshaped_tps_value = np.expand_dims(reshaped_tps_value, axis=0)\n","  reshaped_tps_value = torch.from_numpy(reshaped_tps_value).float().to(device)\n","\n","  src_mask_test = get_src_mask(BATCH_SIZE_TEST, INPUT_LEN).to(device)\n","  trg_mask_test = get_trg_mask(BATCH_SIZE_TEST, PRED_LEN).to(device)\n","\n","  transformer_out = transformer_best_model(reshaped_tps_value, reshaped_tps_value, src_mask_test, trg_mask_test, PRED_LEN).squeeze(0)\n","\n","  output_1 = pd.DataFrame(transformer_out.cpu().detach().numpy())\n","  st = horizon[test_file][0]; et = horizon[test_file][1]\n","  print (st, et)\n","  output_1.index = pd.date_range(start=st, end=et, freq='15min').astype(int) / 10**9\n","\n","  # output_1.index = pd.date_range(start='2020-06-02 06:15:00', end='2020-06-02 09:00:00', freq='15min').astype(int) / 10**9\n","  output_1.columns = reshaped_tps_df.columns\n","\n","  # result = pd.concat([output_1, output_2, ..., output_15 ])\n","  # result = pd.concat([output_1])\n","  out_df = pd.concat([out_df,output_1])\n","out_df.to_json('./traffic_forecasting_transformer.json')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5so2uf-pCD6E","executionInfo":{"status":"ok","timestamp":1656253021152,"user_tz":300,"elapsed":12813,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}},"outputId":"6d3e37e1-070e-4edf-8ff1-6785312f9d43"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["test_data/tps_1.pkl\n","2020-06-02 06:15:00 2020-06-02 09:00:00\n","test_data/tps_2.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-03 07:15:00 2020-06-03 10:00:00\n","test_data/tps_3.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-04 08:15:00 2020-06-04 11:00:00\n","test_data/tps_4.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-05 09:15:00 2020-06-05 12:00:00\n","test_data/tps_5.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-06 10:15:00 2020-06-06 13:00:00\n","test_data/tps_6.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-07 11:15:00 2020-06-07 14:00:00\n","test_data/tps_7.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-08 12:15:00 2020-06-08 15:00:00\n","test_data/tps_8.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-09 13:15:00 2020-06-09 16:00:00\n","test_data/tps_9.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-10 14:15:00 2020-06-10 17:00:00\n","test_data/tps_10.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-11 15:15:00 2020-06-11 18:00:00\n","test_data/tps_11.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-12 16:15:00 2020-06-12 19:00:00\n","test_data/tps_12.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-13 17:15:00 2020-06-13 20:00:00\n","test_data/tps_13.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-14 18:15:00 2020-06-14 21:00:00\n","test_data/tps_14.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-15 19:15:00 2020-06-15 22:00:00\n","test_data/tps_15.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-16 20:15:00 2020-06-16 23:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]}]}]}