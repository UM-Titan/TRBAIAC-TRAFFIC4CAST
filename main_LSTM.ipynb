{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_LSTM.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"19W6bPqrQ00mIaNppKjqELA6VdmC6hvah","authorship_tag":"ABX9TyOVjgEUx55soQkZQWlrT5CO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fpfKUbZUY8o","executionInfo":{"status":"ok","timestamp":1656245119413,"user_tz":300,"elapsed":333,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}},"outputId":"7a02235e-88b7-4c5d-bec5-ccb53a1acaca"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/dev/aiac/TRBAIAC-TRAFFIC\n"]}],"source":["%cd '/content/drive/MyDrive/dev/aiac/TRBAIAC-TRAFFIC'"]},{"cell_type":"code","source":["import torch,os\n","import torch.nn as nn\n","import torch.utils.data as utils\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import time\n","from torch.autograd import Variable"],"metadata":{"id":"6mTuKwK8UdYW","executionInfo":{"status":"ok","timestamp":1656245113670,"user_tz":300,"elapsed":315,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Download training, testing and shapefile data\n","!gdown 'https://drive.google.com/uc?id=1E_qqe7kfvfApM4hCOBMPoXhyEPyrUJkN'\n","!gdown 'https://drive.google.com/uc?id=1j-3-lHegY--FDHKZvz86HBtilV2dsP-i'\n","!gdown 'https://drive.google.com/uc?id=1AhAb3yEVxcHSaz0ADlyyYwNH2Gqmr-um'\n","!unzip 'train.zip' -d '.'\n","!unzip 'test.zip' -d '.'\n","!unzip 'Geoshapefile.zip' -d '.'"],"metadata":{"id":"WZUKc2rVdSJt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Preparation\n","def reshape_data(rawdata):\n","    reshaped_tps_df = pd.DataFrame()\n","    reshaped_tps_df['TIME'] = rawdata.time.unique()\n","    for seg in rawdata.segmentID.unique():\n","        column = rawdata[rawdata['segmentID'] == seg][['time','TrafficIndex_GP']].drop_duplicates(subset=['time'])\n","        column.columns = ['TIME', str(seg)]\n","        reshaped_tps_df = reshaped_tps_df.join(column.set_index('TIME'), on='TIME')\n","\n","    return reshaped_tps_df\n","\n","def load_data(filepath, start_time, end_time, freq):\n","    rawdata = pd.read_pickle(filepath)\n","    matrix = reshape_data(rawdata)\n","    matrix['TIME'] = pd.to_datetime(matrix['TIME']).dt.strftime('%Y-%m-%d %H:%M:%S')\n","\n","    dt_idx = pd.date_range(start=start_time, end=end_time, freq=freq)\n","\n","    output = pd.DataFrame(dt_idx)\n","    output.columns = ['TIME']\n","    output['TIME'] = pd.to_datetime(output['TIME']).dt.strftime('%Y-%m-%d %H:%M:%S')\n","    output = output.set_index('TIME').join(matrix.set_index('TIME'))\n","\n","    return output\n","\n","def prepare_dataloader(matrix, n_col, seq_len=36, pred_len=12, BATCH_SIZE=32, device='cpu'):\n","    seg = matrix.columns.values\n","    time = matrix.index.values\n","    n_seg = len(seg)\n","    n_time = len(time)\n","    \n","    speedMatrix = matrix.to_numpy()\n","    \n","    data_set = []\n","    label_set = []\n","\n","    for i in range(n_time - seq_len - pred_len):\n","        data = speedMatrix[i : i + seq_len]\n","        \n","        label_data = speedMatrix[i + seq_len: i + seq_len + pred_len, :n_col]\n","        \n","        if np.isnan(np.sum(data[:n_col])).any() | np.isnan(np.sum(label_data)):\n","            pass\n","        else:\n","\n","            data_set.append(data)\n","            label_set.append(label_data)\n","            \n","    data = np.array(data_set)\n","    label = np.array(label_set)\n","\n","    train_ind = int(len(data)* 0.8)\n","    valid_ind = int(len(data) * 0.9)\n","    test_ind = int(len(data) * 1.0)\n","\n","    X_train = data[: train_ind]\n","    X_valid = data[train_ind : valid_ind]\n","    X_test = data[valid_ind : test_ind]\n","    Y_train = label[: train_ind]\n","    Y_valid = label[train_ind : valid_ind]\n","    Y_test = label[valid_ind : test_ind]\n","\n","    X_train = torch.FloatTensor(X_train).to(device)\n","    X_valid = torch.FloatTensor(X_valid).to(device)\n","    X_test = torch.FloatTensor(X_test).to(device)\n","    Y_train = torch.FloatTensor(Y_train).to(device)\n","    Y_valid = torch.FloatTensor(Y_valid).to(device)\n","    Y_test = torch.FloatTensor(Y_test).to(device)\n","\n","    train_dataset = utils.TensorDataset(X_train, Y_train)\n","    valid_dataset = utils.TensorDataset(X_valid, Y_valid)\n","    test_dataset = utils.TensorDataset(X_test, Y_test)\n","\n","    train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n","    valid_dataloader = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n","    test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False, drop_last = False)\n","\n","    return train_dataloader, valid_dataloader, test_dataloader"],"metadata":{"id":"IySkBxQ-UanH","executionInfo":{"status":"ok","timestamp":1656245113799,"user_tz":300,"elapsed":135,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Model Implementation\n","class lstm_encoder(nn.Module):\n","    ''' Encodes time-series sequence '''\n","\n","    def __init__(self, input_size, hidden_size, num_layers = 2):\n","        \n","        super(lstm_encoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n","                            num_layers=num_layers, batch_first=True)\n","\n","    def forward(self, x_input):\n","        \n","        use_gpu = torch.cuda.is_available()\n","        if use_gpu:\n","            Hidden_State = Variable(torch.zeros(self.num_layers, x_input.size(0), self.hidden_size).cuda())\n","            Cell_State = Variable(torch.zeros(self.num_layers, x_input.size(0), self.hidden_size).cuda())\n","        else:\n","            Hidden_State = Variable(torch.zeros(self.num_layers, x_input.size(0), self.hidden_size))\n","            Cell_State = Variable(torch.zeros(self.num_layers, x_input.size(0), self.hidden_size))\n","        \n","        lstm_out, self.hidden = self.lstm(x_input, (Hidden_State, Cell_State))\n","        return lstm_out, self.hidden\n","\n","class lstm_decoder(nn.Module):\n","    \n","    def __init__(self, input_size, hidden_size, num_layers = 2):\n","        \n","        super(lstm_decoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n","                            num_layers=num_layers, batch_first=True)       \n","\n","    def forward(self, x_input, encoder_hidden_states):\n","        lstm_out, self.hidden = self.lstm(x_input.unsqueeze(1), encoder_hidden_states)      \n","        return lstm_out.squeeze(1), self.hidden\n","\n","class LSTM_Seq2Seq(nn.Module):\n","\n","    def __init__(self, encoder, decoder, target_len):\n","        super(LSTM_Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.target_len = target_len\n","        \n","    def forward(self, inputs):\n","    \n","        batch_size = inputs.shape[0]\n","        output_feature = inputs.shape[2]\n","        outputs = None\n","    \n","        encoder_output, encoder_hidden = self.encoder(inputs)\n","        decoder_input = inputs[:, -1, :]\n","        decoder_hidden = encoder_hidden\n","                \n","        for t in range(self.target_len): \n","            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n","            if outputs is None:\n","                outputs = decoder_output.unsqueeze(1)\n","            else:\n","                outputs = torch.cat((decoder_output.unsqueeze(1), outputs), 1)\n","            decoder_input = decoder_output\n","        return outputs\n","\n","def TrainModel_LSTM_Seq2Seq(model, train_dataloader, valid_dataloader, learning_rate, num_epochs, min_delta, use_gpu, patience):\n","    \n","    loss_MSE = torch.nn.MSELoss()\n","    optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n","    \n","    cur_time = time.time()\n","    pre_time = time.time()\n","\n","    # Variables for Early Stopping\n","    is_best_model = 0\n","    patient_epoch = 0\n","       \n","    for epoch in range(num_epochs):\n","\n","        total_train_loss = 0\n","        total_valid_loss = 0\n","\n","        for data in train_dataloader:\n","\n","            inputs, labels = data\n","            batch_size = inputs.shape[0]\n","            \n","            if inputs.shape[0] != batch_size:\n","                continue\n","                \n","            if use_gpu:\n","                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n","            else:\n","                inputs, labels = Variable(inputs), Variable(labels)\n","\n","            model.train()\n","            outputs = model(inputs)\n","\n","            loss_train = loss_MSE(outputs, torch.squeeze(labels))\n","            total_train_loss += loss_train.data\n","\n","            optimizer.zero_grad()\n","            loss_train.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","\n","        for data in valid_dataloader:\n","\n","            inputs, labels = data\n","            batch_size = inputs.shape[0]\n","            \n","            if inputs.shape[0] != batch_size:\n","                continue\n","                \n","            if use_gpu:\n","                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n","            else:\n","                inputs, labels = Variable(inputs), Variable(labels)\n","  \n","            outputs_val= model(inputs)\n","            loss_valid = loss_MSE(outputs_val, torch.squeeze(labels))\n","            total_valid_loss += loss_valid.data\n","\n","        avg_losses_epoch_train = total_train_loss / float(len(train_dataloader))\n","        avg_losses_epoch_valid = total_valid_loss / float(len(valid_dataloader))\n","\n","        # Early Stopping\n","        if epoch == 0:\n","            is_best_model = 1\n","            best_model = model\n","            min_loss_epoch_valid = 10000.0\n","            if avg_losses_epoch_valid < min_loss_epoch_valid:\n","                min_loss_epoch_valid = avg_losses_epoch_valid\n","        else:\n","            if min_loss_epoch_valid - avg_losses_epoch_valid > min_delta:\n","                is_best_model = 1\n","                best_model = model\n","                min_loss_epoch_valid = avg_losses_epoch_valid\n","                patient_epoch = 0\n","            else:\n","                is_best_model = 0\n","                patient_epoch += 1\n","                if patient_epoch >= patience:\n","                    print('Early Stopped at Epoch:', epoch)\n","                    break\n","\n","        cur_time = time.time()\n","        print('Epoch: {}, train_loss: {}, valid_loss: {}, time: {}, best model: {}'.format( \\\n","                    epoch, \\\n","                    np.around(avg_losses_epoch_train.cpu(), decimals=8),\\\n","                    np.around(avg_losses_epoch_valid.cpu(), decimals=8),\\\n","                    np.around([cur_time - pre_time] , decimals=2),\\\n","                    is_best_model) )\n","        pre_time = cur_time\n","\n","    return best_model"],"metadata":{"id":"UA7r7f3PUwGr","executionInfo":{"status":"ok","timestamp":1656245113800,"user_tz":300,"elapsed":136,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 40\n","INPUT_LEN = 36\n","PRED_LEN = 12\n","LEARNING_RATE = 1e-5\n","NUM_EPOCHS = 100\n","MIN_DELTA = 5e-4\n","PATIENCE = 10\n","\n","BASELINE_MODEL_INPUT_DIMENSION = 87\n","BASELINE_MODEL_HIDDEN_DIMENSION = 87\n","BASELINE_MODEL_OUTPUT_DIMENSION = 87"],"metadata":{"id":"JGyW037jU5AN","executionInfo":{"status":"ok","timestamp":1656245128506,"user_tz":300,"elapsed":177,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","use_gpu = torch.cuda.is_available()"],"metadata":{"id":"qYbH1xsiVDYR","executionInfo":{"status":"ok","timestamp":1656245130025,"user_tz":300,"elapsed":262,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(use_gpu)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"50JKIUmLVE9j","executionInfo":{"status":"ok","timestamp":1656245131676,"user_tz":300,"elapsed":346,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}},"outputId":"a47ee4b7-9bd9-4f6a-c415-bdad2d9080d7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["data_matrix = load_data(\"./tps_df.pkl\", \"2020-01-01 00:00:00.000\", \"2020-05-31 23:45:00.000\", freq=\"15min\")\n","input_dim = data_matrix.shape[-1]\n","train_dataloader, valid_dataloader, _ = prepare_dataloader(data_matrix, input_dim, BATCH_SIZE=BATCH_SIZE, seq_len=INPUT_LEN, pred_len=PRED_LEN, device=device)"],"metadata":{"id":"g2j8BvpDVUpg","executionInfo":{"status":"ok","timestamp":1656245148139,"user_tz":300,"elapsed":13324,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["encoder = lstm_encoder(input_size=BASELINE_MODEL_INPUT_DIMENSION, hidden_size=BASELINE_MODEL_HIDDEN_DIMENSION)\n","decoder = lstm_decoder(input_size=BASELINE_MODEL_HIDDEN_DIMENSION, hidden_size=BASELINE_MODEL_OUTPUT_DIMENSION)\n","seq2seq_model = LSTM_Seq2Seq(encoder, decoder, PRED_LEN).to(device)"],"metadata":{"id":"WLpyGQjxV2CP","executionInfo":{"status":"ok","timestamp":1656245148139,"user_tz":300,"elapsed":17,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["seq2seq_best_model = TrainModel_LSTM_Seq2Seq(seq2seq_model, train_dataloader, valid_dataloader, num_epochs=NUM_EPOCHS, \n","                                              learning_rate=LEARNING_RATE, \n","                                              min_delta=MIN_DELTA, \n","                                              use_gpu=use_gpu,\n","                                              patience=PATIENCE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m8JarD1kV2Kh","executionInfo":{"status":"ok","timestamp":1656245227311,"user_tz":300,"elapsed":79188,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}},"outputId":"b2fc8b7b-656f-498b-d136-6a8c3187f39a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, train_loss: 0.8676244020462036, valid_loss: 0.7941045761108398, time: [2.98], best model: 1\n","Epoch: 1, train_loss: 0.5174614191055298, valid_loss: 0.33493682742118835, time: [2.87], best model: 1\n","Epoch: 2, train_loss: 0.21901479363441467, valid_loss: 0.15726694464683533, time: [2.86], best model: 1\n","Epoch: 3, train_loss: 0.11146687716245651, valid_loss: 0.08382993191480637, time: [2.89], best model: 1\n","Epoch: 4, train_loss: 0.06320104002952576, valid_loss: 0.0451960414648056, time: [2.91], best model: 1\n","Epoch: 5, train_loss: 0.038029540330171585, valid_loss: 0.02455786056816578, time: [2.88], best model: 1\n","Epoch: 6, train_loss: 0.024972179904580116, valid_loss: 0.013418110087513924, time: [3.48], best model: 1\n","Epoch: 7, train_loss: 0.018265610560774803, valid_loss: 0.007635519839823246, time: [4.35], best model: 1\n","Epoch: 8, train_loss: 0.015055689960718155, valid_loss: 0.00472575007006526, time: [4.85], best model: 1\n","Epoch: 9, train_loss: 0.013519249856472015, valid_loss: 0.003445930080488324, time: [2.87], best model: 1\n","Epoch: 10, train_loss: 0.012095009908080101, valid_loss: 0.0026748899836093187, time: [2.84], best model: 1\n","Epoch: 11, train_loss: 0.010721559636294842, valid_loss: 0.0020223299507051706, time: [2.86], best model: 1\n","Epoch: 12, train_loss: 0.009878389537334442, valid_loss: 0.0016584000550210476, time: [2.87], best model: 0\n","Epoch: 13, train_loss: 0.009407940320670605, valid_loss: 0.0014362899819388986, time: [2.85], best model: 1\n","Epoch: 14, train_loss: 0.009122270159423351, valid_loss: 0.0012642400106415153, time: [2.93], best model: 0\n","Epoch: 15, train_loss: 0.008913679979741573, valid_loss: 0.0012881900183856487, time: [4.29], best model: 0\n","Epoch: 16, train_loss: 0.008747410029172897, valid_loss: 0.0011894600465893745, time: [4.75], best model: 0\n","Epoch: 17, train_loss: 0.008570419624447823, valid_loss: 0.0011456599459052086, time: [3.79], best model: 0\n","Epoch: 18, train_loss: 0.008387619629502296, valid_loss: 0.0011376499896869063, time: [4.54], best model: 0\n","Epoch: 19, train_loss: 0.008146779611706734, valid_loss: 0.0012186700478196144, time: [2.99], best model: 0\n","Epoch: 20, train_loss: 0.007877659983932972, valid_loss: 0.0012476800475269556, time: [2.85], best model: 0\n","Epoch: 21, train_loss: 0.007614240050315857, valid_loss: 0.0013995099579915404, time: [2.86], best model: 0\n","Epoch: 22, train_loss: 0.007378859911113977, valid_loss: 0.0012197500327602029, time: [2.87], best model: 0\n","Early Stopped at Epoch: 23\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"57J6smUwdQ03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Step1: read and reshape data\n","Step2: feed into trained model\n","Step3: convert output to JSON file\n","Here we take one testing data as an example to convert it to JSON file.\n","In this challenge, you should convert all 15 predicting results to one JSON file.\n","Please check https://colab.research.google.com/drive/1Hkt3kQuh7WzwUTnLgKCcvfPAV1CUK8lF?usp=sharing for more information of the expected result.\n","'''\n","test_files = os.listdir('test_data')\n","out_cols = ['11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n","       '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n","       '35', '36', '37', '38', '39', '40', '42', '43', '44', '45', '46', '47',\n","       '49', '50', '51', '52', '53', '54', '55', '58', '59', '60', '61', '62',\n","       '63', '64', '65', '66', '67', '68', '69', '70', '73', '74', '75', '76',\n","       '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n","       '89', '90', '91', '92', '93', '94', '95', '96', '97', '99', '100',\n","       '102', '103', '104', '106']\n","\n","horizon = {'tps_1.pkl':['2020-06-02 06:15:00', '2020-06-02 09:00:00'],\n","           'tps_2.pkl':['2020-06-03 07:15:00', '2020-06-03 10:00:00'],\n","           'tps_3.pkl':['2020-06-04 08:15:00', '2020-06-04 11:00:00'],\n","           'tps_4.pkl':['2020-06-05 09:15:00', '2020-06-05 12:00:00'],\n","           'tps_5.pkl':['2020-06-06 10:15:00', '2020-06-06 13:00:00'],\n","           'tps_6.pkl':['2020-06-07 11:15:00', '2020-06-07 14:00:00'],\n","           'tps_7.pkl':['2020-06-08 12:15:00', '2020-06-08 15:00:00'],\n","           'tps_8.pkl':['2020-06-09 13:15:00', '2020-06-09 16:00:00'],\n","           'tps_9.pkl':['2020-06-10 14:15:00', '2020-06-10 17:00:00'],\n","           'tps_10.pkl':['2020-06-11 15:15:00', '2020-06-11 18:00:00'],\n","           'tps_11.pkl':['2020-06-12 16:15:00', '2020-06-12 19:00:00'],\n","           'tps_12.pkl':['2020-06-13 17:15:00', '2020-06-13 20:00:00'],\n","           'tps_13.pkl':['2020-06-14 18:15:00', '2020-06-14 21:00:00'],\n","           'tps_14.pkl':['2020-06-15 19:15:00', '2020-06-15 22:00:00'],\n","           'tps_15.pkl':['2020-06-16 20:15:00', '2020-06-16 23:00:00'],\n","           }\n","out_df = pd.DataFrame(columns = out_cols)\n","for test_file in test_files:\n","  print (os.path.join('test_data',test_file))\n","  tps_1_raw = pd.read_pickle(os.path.join('test_data',test_file))\n","  # print (tps_1_raw.head())\n","  reshaped_tps_df = reshape_data(tps_1_raw)\n","  reshaped_tps_df = reshaped_tps_df.set_index('TIME')\n","  reshaped_tps_value = reshaped_tps_df.values\n","  reshaped_tps_value = np.expand_dims(reshaped_tps_value, axis=0)\n","  reshaped_tps_value = torch.from_numpy(reshaped_tps_value).float().to(device)\n","\n","  seq2seq_out = seq2seq_best_model(reshaped_tps_value).squeeze(0)\n","\n","  output_1 = pd.DataFrame(seq2seq_out.cpu().detach().numpy())\n","  # output_1.index = pd.date_range(start='2020-06-02 06:15:00', end='2020-06-02 09:00:00', freq='15min').astype(int) / 10**9\n","  st = horizon[test_file][0]; et = horizon[test_file][1]\n","  print (st, et)\n","  output_1.index = pd.date_range(start=st, end=et, freq='15min').astype(int) / 10**9\n","  output_1.columns = reshaped_tps_df.columns\n","  \n","  out_df = pd.concat([out_df,output_1])\n","\n","# out_df.reset_index(inplace=True)\n","out_df.to_json('./traffic_forecasting_result_all.json')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7IvgUbuWpxL","executionInfo":{"status":"ok","timestamp":1656246308333,"user_tz":300,"elapsed":7014,"user":{"displayName":"Yaw Adu-Gyamfi","userId":"06648105294395810325"}},"outputId":"980701d3-2a02-4cc7-ec25-cb4be9f26b90"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["test_data/tps_1.pkl\n","2020-06-02 06:15:00 2020-06-02 09:00:00\n","test_data/tps_2.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-03 07:15:00 2020-06-03 10:00:00\n","test_data/tps_3.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-04 08:15:00 2020-06-04 11:00:00\n","test_data/tps_4.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-05 09:15:00 2020-06-05 12:00:00\n","test_data/tps_5.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-06 10:15:00 2020-06-06 13:00:00\n","test_data/tps_6.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-07 11:15:00 2020-06-07 14:00:00\n","test_data/tps_7.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-08 12:15:00 2020-06-08 15:00:00\n","test_data/tps_8.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-09 13:15:00 2020-06-09 16:00:00\n","test_data/tps_9.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-10 14:15:00 2020-06-10 17:00:00\n","test_data/tps_10.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-11 15:15:00 2020-06-11 18:00:00\n","test_data/tps_11.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-12 16:15:00 2020-06-12 19:00:00\n","test_data/tps_12.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-13 17:15:00 2020-06-13 20:00:00\n","test_data/tps_13.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-14 18:15:00 2020-06-14 21:00:00\n","test_data/tps_14.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-15 19:15:00 2020-06-15 22:00:00\n","test_data/tps_15.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]},{"output_type":"stream","name":"stdout","text":["2020-06-16 20:15:00 2020-06-16 23:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"-5KfBW7cbbY5"},"execution_count":null,"outputs":[]}]}